---
layout: post
title:  "AI Adoption And Individual Performance Metrics"
date:   2025-12-11 08:00:00 -0400
categories: work
---
# AI Adoption And Individual Performance Metrics

Leadership at many companies is driving AI adoption.

AI offers a great opportunity to speed up development time.

But it would be a mistake to use an individual's AI usage statistics as a performance metric for three reasons:

- The way developers are meant to use AI is not yet well defined
- Goodhart’s law
- Brandolini’s law

## The way to use AI is not yet well-defined

- Should developers use it to answer questions? 
- Write tests? 
- Write production code? 
- Do code reviews? 
- Do tasks independently?
- How many layers of prompts and instructions files are needed to get the best output? How much effort should developers put into them?

It does not seem right that a developer’s productivity is measured by how much they use a tool whose purpose has not been well-defined.

It’s okay to reward a few for innovation and knowledge sharing, but it doesn’t seem fair to penalize.

## Goodhart’s Law

**_When a measure becomes the target, it ceases to be a good measure._**

AI usage is not a customer-facing metric. Customers don’t know whether a developer used AI to develop a feature or not.

If leadership wants to measure how much a developer uses AI, the developer will just find some way to game the system to bump their numbers up.

This is like measuring productivity by the number of lines of code a developer writes, the number nails a carpenter hammers, or the number of words an author writes in a book. These isolated statistics mean nothing.

## Brandolini’s Law

Developers might not be keen on maximizing their AI usage because it generates a lot of material that they may not want to be responsible for. 

**_The effort to refute bullshit is an order of magnitude greater than the effort to produce it. _**

In an age of AI where bullshit is easier than ever to produce, how much of an opportunity should a developer be willing to give to AI? 

For example; if a developer has AI do a code review, and it inaccurately finds a number of issues in the code (and, sure, we can blame the developer for prompting it “incorrectly") to what extent does the developer now have an obligation to explain why those findings are inaccurate? If the developer can privately decide for herself, good. But if those code review files are held on to in any fashion (whether committed to a repository or shared elsewhere) will she then be held to account for responding to everything the AI outputs?

As another example; if a developer decides that they want to have AI write a mountain of documentation about an aspect of a codebase, that could easily bury documentation about a more important aspect. Shall we then have AI read the documentation for us as well?

So the question becomes; _“Do I want to be responsible for a potential mountain that AI generates in 5 minutes? Or would I prefer to spend an hour or two on something, even if it turns out to be a mole hill?”_ Perhaps the latter!

## Why we care

The issue isn’t with the quality of code that AI generates (which is usually as good as or better than most code that humans write).

The issue is with the quantity that it generates, and the reckless application that the individual performance metric would encourage.

This is not just an issue at one company, but is a broad issue for the software engineering community.

## An alternative way to measure productivity

Software engineering has always been difficult to measure. (This was particularly well documented last year by controversy generated by the [McKinsey article](https://www.mckinsey.com/industries/technology-media-and-telecommunications/our-insights/yes-you-can-measure-software-developer-productivity) and responses to it from [Kent Beck](https://newsletter.pragmaticengineer.com/p/measuring-developer-productivity) and [Dave Farly](https://www.youtube.com/watch?v=5VpBRmmWqg0)).

Measurement is not offensive to a proper engineer; but there are better ways to measure productivity. Dave Farly, a trusted voice in the software engineering community, recommends Google’s [DORA framework](https://dora.dev/).

In this case, our limited resources are developer expertise and computation time.

In this context, we have to know who our developers are, and how much data pass through the system. If we knew those two things, we could make our best, though still imperfect, decision.
